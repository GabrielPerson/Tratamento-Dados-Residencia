{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASKS HANDS ON UNIMED\n",
    "\n",
    "***\n",
    "\n",
    "SEARCH BASE W/ MISSING VALUES\n",
    "- UCI ML REPO\n",
    "- Datasets: Titanic.SCV & SkillCraft1_Dataset.csv\n",
    "- More missing values, randomize. -- DONE\n",
    "\n",
    "\n",
    "DATASETS DIFERENTES PARA CADA TAREFA\n",
    "\n",
    "HANDS ON:\n",
    "- Task 1 - fill.na\n",
    "            . Use drop, fill(mean), fill(median)\n",
    "            . Compare results\n",
    "- norm [0,1] &  PCA\n",
    "- Norm. Gaussiana & PCA      \n",
    "- check diff entre normalizações\n",
    "- Discretização de valores\n",
    "- feature selec. \n",
    "        . information gain - decision tree\n",
    "        . gini - random forest\n",
    "        . isomap\n",
    "        . tsne\n",
    "- compare FeatSelec & Disc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](unimed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercícios sobre tratamendo de dados\n",
    "\n",
    "Descrição dos Dados:   https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "\n",
    "Os dados estarão disponíveis no arquivo **winequality-red.csv**\n",
    "\n",
    "\n",
    "Agora vamos botar a mão na massa. Usaremos um novo DataSet para aplicar o que aprendemos hoje: tratamento de valores faltantes, discretização, normalização, PCA, Feature Selection e Dimensionality Reduction. Usaremos bilbiotecas como numpy, pandas e scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos botar a mão na massa. Usaremos um novo DataSet para aplicar o que aprendemos hoje: tratamento de valores faltantes, discretização, normalização, PCA, Feature Selection e Dimensionality Reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas e Carregamento do DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agora, leia o arquivo CSV winequality-red.csv e salve em uma variável'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Importe as bibliotecas que usaremos,como numpy, pandas, matplotib, etc\"\"\"\n",
    "#YOUR CODE HERE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "\"\"\"Agora, leia o arquivo CSV winequality-red.csv e salve em uma variável\"\"\"\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja agora como são os dados do DataSet e algumas caracteristicas deles. Funções .head( ) e .describe( ) podem ajudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-ser ver que nem todos os dados estão presentes. Temos muitos dados faltantes. Pense na maneira mais adequada de preenche-los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que nossos dados se encontram em dominios muito diferentes. Isso pode ser prejudicial para o modelo que queremo usar. Vamos Normalizá-los utilizando normalização **MaxMin** e **Gaussiana**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos utilizar o PCA para reduzir a dimensionalidade dos nossos dados.\n",
    "\n",
    "O PCA é uma ferramenta importante usada em datasets com muitos atributos, sendo capaz de reduzir sua dimensionalidade analisando seus componentes principais, para que possamos usa-los em nosso modelo garantindo eficiencia sem muita perda de informação.\n",
    "\n",
    "Inicialmente, vamos nos guiar pela documentação da biblioteca, que você pode acessar clicando <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">aqui</a>.\n",
    "\n",
    "Em um primeiro momento, não devemos passar nenhum valor para o parâmetro **n_components**, dessa forma, podemos visualizar todos os componentes principais gerados e posteriormente decidirmos o número de componentes a serem utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A função PCA não admite valores categóricos. Logo devemos retirar do nosso dataset a coluna ['species']\n",
    "que será nosso target\"\"\"\n",
    "\n",
    "data.drop('species',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92461621 0.05301557 0.01718514 0.00518309]\n",
      "[0.92461621 0.97763178 0.99481691 1.        ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Importe a função PCA da biblioteca sklearn\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\"\"\"Crie uma variável pca que receba a função PCA(), em seguida treine o modelo com seus dados \n",
    "e analise a porcentagem de variância explicada por cada componente principal.\n",
    "Dica: arrays possuem a função .cumsum() que permite visualizar o valor acumulativos de seus itens.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseando-se na análise realizada acima, já podemos reduzir a dimensionalidade de nossos dados. Agora o parâmetro **n_components** deve ser alterado recebendo como valor o número de componentes principais necessários para explicar no mínimo 97% da variância dos dados. Após a alteração do parâmetro, o modelo deve ser retreinado e em seguida, podemos realizar a transformação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Crie uma nova variável chamada data_pca, a variável deverá receber os dados transformados pelo PCA,\n",
    "contendo componentes principais suficientes para explicar 97% da variância dos dados.\"\"\"\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(df_fill_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SkillCraft1_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padronização é legal.\n",
    "Processsamento e normalização ajuda o modelo de ML\n",
    "<img src='./img/data_scale.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "data_scaled = pd.DataFrame(x_scaled,columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0      0.222222     0.625000      0.067797     0.041667\n",
       "1      0.166667     0.416667      0.067797     0.041667\n",
       "2      0.111111     0.500000      0.050847     0.041667\n",
       "3      0.083333     0.458333      0.084746     0.041667\n",
       "4      0.194444     0.666667      0.067797     0.041667"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização Gaussiana (Z-score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
